{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∏ –∏–º–ø–æ—Ä—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –±–∏–±–ª–∏–æ—Ç–µ–∫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mg821\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from pymorphy3 import MorphAnalyzer\n",
    "from nltk.stem import SnowballStemmer\n",
    "from googletrans import Translator\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∑–∫–∞ —Å—Ç–æ–ø-—Å–ª–æ–≤ –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ó–∞–≥—Ä—É–∑–∫–∞ —Å—Ç–æ–ø-—Å–ª–æ–≤ –∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('russian'))\n",
    "\n",
    "# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞ –∏ —Å—Ç–µ–º–º–µ—Ä–∞\n",
    "morph = MorphAnalyzer()\n",
    "stemmer = SnowballStemmer(\"russian\")\n",
    "\n",
    "# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–µ—Ä–µ–≤–æ–¥—á–∏–∫–∞\n",
    "translator = Translator()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "—Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def lemmatize_word(word):\n",
    "    \"\"\"–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ª–µ–º–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é —Ñ–æ—Ä–º—É —Å–ª–æ–≤–∞.\"\"\"\n",
    "    return morph.parse(word)[0].normal_form\n",
    "\n",
    "def stem_word(word):\n",
    "    \"\"\"–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–µ–º–º–∏—Ä–æ–≤–∞–Ω–Ω—É—é —Ñ–æ—Ä–º—É —Å–ª–æ–≤–∞.\"\"\"\n",
    "    return stemmer.stem(word)\n",
    "\n",
    "def process_text(text):\n",
    "    \"\"\"–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç: –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è, —Å—Ç–µ–º–º–∏–Ω–≥ –∏ —É–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–æ–ø-—Å–ª–æ–≤.\"\"\"\n",
    "    \n",
    "    words = text.split()\n",
    "    # –£–±–∏—Ä–∞–µ–º —Å—Ç–æ–ø-—Å–ª–æ–≤–∞\n",
    "    filtered_words = [word for word in words if word not in stop_words]\n",
    "    \n",
    "    lemmatized_words = [lemmatize_word(word) for word in filtered_words]\n",
    "    stemmed_words = [stem_word(word) for word in filtered_words]\n",
    "    \n",
    "    return lemmatized_words, stemmed_words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " –§—É–Ω–∫—Ü–∏—è –ø–µ—Ä–µ–≤–æ–¥–∞ —Ç–µ–∫—Å—Ç–∞ —Å —Ä—É—Å—Å–∫–æ–≥–æ –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–∏–π, –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –∏ –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_to_english(text):\n",
    "    \"\"\"–ü–µ—Ä–µ–≤–æ–¥–∏—Ç —Ç–µ–∫—Å—Ç —Å —Ä—É—Å—Å–∫–æ–≥–æ –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–∏–π.\"\"\"\n",
    "    translation = translator.translate(text, src='ru', dest='en')\n",
    "    return translation.text\n",
    "\n",
    "def tokenize_text(text):\n",
    "    \"\"\"–¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ—Ç —Ç–µ–∫—Å—Ç –Ω–∞ —É—Ä–æ–≤–Ω–µ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤.\"\"\"\n",
    "    return list(text)\n",
    "\n",
    "def vectorize_text(text):\n",
    "    \"\"\"–í–µ–∫—Ç–æ—Ä–∏–∑—É–µ—Ç —Ç–µ–∫—Å—Ç, –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—è –∫–∞–∂–¥—ã–π —Å–∏–º–≤–æ–ª –≤ –≤–∏–¥–µ –µ–≥–æ ASCII-–∫–æ–¥–∞.\"\"\"\n",
    " \n",
    "    # –ü—Ä–∏–≤–æ–¥–∏–º —Ç–µ–∫—Å—Ç –∫ ASCII (–æ—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ —Å–∏–º–≤–æ–ª—ã —Å –∫–æ–¥–∞–º–∏ < 128)\n",
    "    ascii_text = ''.join([char if ord(char) < 128 else ' ' for char in text])\n",
    "    \n",
    "    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –∫–∞–∂–¥—ã–π —Å–∏–º–≤–æ–ª –≤ –µ–≥–æ ASCII –∫–æ–¥\n",
    "    ascii_codes = [ord(char) for char in ascii_text]\n",
    "    \n",
    "    return ascii_codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ò—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç\n",
    "text = \"—Å–∞—à–∞ —à–ª–∞ –ø–æ —à–æ—Å—Å–µ –∏ —Å–æ—Å–∞–ª–∞ —Å—É—à–∫—É\"\n",
    "\n",
    "# –õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è –∏ —Å—Ç–µ–º–º–∏–Ω–≥\n",
    "lemmatized_text, stemmed_text = process_text(text)\n",
    "\n",
    "# –ü–µ—Ä–µ–≤–æ–¥ –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–∏–π\n",
    "translated_lemmatized_text = translate_to_english(' '.join(lemmatized_text))\n",
    "translated_stemmed_text = translate_to_english(' '.join(stemmed_text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –∏ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Ä—É—Å—Å–∫–∞—è\n",
    "ru_tokenized_lemmatized_text = tokenize_text(' '.join(lemmatized_text))\n",
    "ru_tokenized_stemmed_text = tokenize_text(' '.join(stemmed_text))\n",
    "\n",
    "# –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –∏ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –∞–Ω–≥–ª–∏–π—Å–∫–∞—è\n",
    "tokenized_lemmatized_text = tokenize_text(translated_lemmatized_text)\n",
    "tokenized_stemmed_text = tokenize_text(translated_stemmed_text)\n",
    "\n",
    "vectorized_lemmatized_text = vectorize_text(translated_lemmatized_text)\n",
    "vectorized_stemmed_text = vectorize_text(translated_stemmed_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "–õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è: ['—Å–∞—à–∞', '–∏–¥—Ç–∏', '—à–æ—Å—Å–µ', '—Å–æ—Å–∞—Ç—å', '—Å—É—à–∫–∞']\n",
      "–°—Ç–µ–º–º–∏–Ω–≥: ['—Å–∞—à', '—à–ª–∞', '—à–æ—Å—Å', '—Å–æ—Å–∞', '—Å—É—à–∫']\n",
      "----------------------------------------\n",
      "\n",
      "üîπ –†—É—Å—Å–∫–∞—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è:\n",
      "–õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è: ['—Å', '–∞', '—à', '–∞', ' ', '–∏', '–¥', '—Ç', '–∏', ' ', '—à', '–æ', '—Å', '—Å', '–µ', ' ', '—Å', '–æ', '—Å', '–∞', '—Ç', '—å', ' ', '—Å', '—É', '—à', '–∫', '–∞']\n",
      "–°—Ç–µ–º–º–∏–Ω–≥: ['—Å', '–∞', '—à', ' ', '—à', '–ª', '–∞', ' ', '—à', '–æ', '—Å', '—Å', ' ', '—Å', '–æ', '—Å', '–∞', ' ', '—Å', '—É', '—à', '–∫']\n",
      "\n",
      "üîπ –ê–Ω–≥–ª–∏–π—Å–∫–∞—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è:\n",
      "–õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è: ['S', 'a', 's', 'h', 'a', ' ', 'g', 'o', ' ', 'h', 'i', 'g', 'h', 'w', 'a', 'y', ' ', 't', 'o', ' ', 's', 'u', 'c', 'k', ' ', 'd', 'r', 'y', 'i', 'n', 'g']\n",
      "–°—Ç–µ–º–º–∏–Ω–≥: ['S', 'a', 's', 'h', ' ', 'w', 'a', 'l', 'k', 'e', 'd', ' ', 'h', 'i', 'g', 'h', 'w', 'a', 'y', ' ', 's', 'u', 'z', 'a', ' ', 's', 'u', 's', 'h', 'k']\n",
      "\n",
      "üîπ –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è:\n",
      "–õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è: [83, 97, 115, 104, 97, 32, 103, 111, 32, 104, 105, 103, 104, 119, 97, 121, 32, 116, 111, 32, 115, 117, 99, 107, 32, 100, 114, 121, 105, 110, 103] ... (–≤—Å–µ–≥–æ 31 —ç–ª–µ–º–µ–Ω—Ç–æ–≤)\n",
      "–°—Ç–µ–º–º–∏–Ω–≥: [83, 97, 115, 104, 32, 119, 97, 108, 107, 101, 100, 32, 104, 105, 103, 104, 119, 97, 121, 32, 115, 117, 122, 97, 32, 115, 117, 115, 104, 107] ... (–≤—Å–µ–≥–æ 30 —ç–ª–µ–º–µ–Ω—Ç–æ–≤)\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"-\" * 40)\n",
    "print(f\"–õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è: {lemmatized_text}\")\n",
    "print(f\"–°—Ç–µ–º–º–∏–Ω–≥: {stemmed_text}\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "print(\"\\nüîπ –†—É—Å—Å–∫–∞—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è:\")\n",
    "print(\"–õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è:\", ru_tokenized_lemmatized_text)\n",
    "print(\"–°—Ç–µ–º–º–∏–Ω–≥:\", ru_tokenized_stemmed_text)\n",
    "\n",
    "print(\"\\nüîπ –ê–Ω–≥–ª–∏–π—Å–∫–∞—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è:\")\n",
    "print(\"–õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è:\", tokenized_lemmatized_text)\n",
    "print(\"–°—Ç–µ–º–º–∏–Ω–≥:\", tokenized_stemmed_text)\n",
    "\n",
    "print(\"\\nüîπ –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è:\")\n",
    "print(\"–õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è:\", vectorized_lemmatized_text, \"...\", f\"(–≤—Å–µ–≥–æ {len(vectorized_lemmatized_text)} —ç–ª–µ–º–µ–Ω—Ç–æ–≤)\")\n",
    "print(\"–°—Ç–µ–º–º–∏–Ω–≥:\", vectorized_stemmed_text, \"...\", f\"(–≤—Å–µ–≥–æ {len(vectorized_stemmed_text)} —ç–ª–µ–º–µ–Ω—Ç–æ–≤)\")\n",
    "print(\"-\" * 40)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
